{
 "cells": [
  {
   "source": [
    "## import packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b741768a-d449-42f0-9290-da4f8964d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, XLMRobertaConfig, XLMRobertaTokenizer\n",
    "from transformers import XLMRobertaModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from itertools import chain\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import logging\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "source": [
    "## set seed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff82ea82-155f-4806-8da9-f101908fc89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n"
   ]
  },
  {
   "source": [
    "## load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cdfb42c-1453-4e3c-895a-d182caddd437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 구성.\n",
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 처음 불러온 tsv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\n",
    "# 변경한 DataFrame 형태는 baseline code description 이미지를 참고해주세요.\n",
    "def preprocessing_dataset(dataset, label_type):\n",
    "    label = []\n",
    "    for i in dataset[8]:\n",
    "        if i == 'blind':\n",
    "            label.append(100)\n",
    "        else:\n",
    "            label.append(label_type[i])\n",
    "    out_dataset = pd.DataFrame({'sentence':dataset[1],'entity_01':dataset[2], 'e1s':dataset[3],'e1e':dataset[4],\n",
    "                              'entity_02':dataset[5], 'e2s':dataset[6],'e2e':dataset[7],'label':label})\n",
    "    return out_dataset\n",
    "\n",
    "# tsv 파일을 불러옵니다.\n",
    "def load_data(dataset_dir):\n",
    "  # load label_type, classes\n",
    "    with open('/opt/ml/input/data/label_type.pkl', 'rb') as f:\n",
    "        label_type = pickle.load(f)\n",
    "  # load dataset\n",
    "    dataset = pd.read_csv(dataset_dir, delimiter='\\t', header=None)\n",
    "  # preprecessing dataset\n",
    "    dataset = preprocessing_dataset(dataset, label_type)\n",
    "  \n",
    "    return dataset\n",
    "\n",
    "# XLMRoberta input을 위한 tokenizing.\n",
    "# tip! 다양한 종류의 tokenizer와 special token들을 활용하는 것으로도 새로운 시도를 해볼 수 있습니다.\n",
    "# baseline code에서는 2가지 부분을 활용했습니다.\n",
    "# def append_token(dataset, tokenizer):\n",
    "#     for (ex_index, example) in enumerate(dataset):\n",
    "        \n",
    "    \n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer):\n",
    "    concat_entity = []\n",
    "    for e01, e02 in zip(dataset['entity_01'], dataset['entity_02']):\n",
    "        temp = ''\n",
    "        temp = e01 + '[SEP]' + e02\n",
    "        concat_entity.append(temp)\n",
    "        tokenized_sentences = tokenizer(\n",
    "      #concat_entity,\n",
    "          list(dataset['sentence']),\n",
    "          return_tensors=\"pt\",\n",
    "          padding=True,\n",
    "          truncation=True,\n",
    "          max_length=150,\n",
    "          add_special_tokens=True,\n",
    "          )\n",
    "    return tokenized_sentences\n",
    "\n",
    "def tokenized_dataset_len(dataset, tokenizer):\n",
    "    li = []\n",
    "    for sentence in dataset['sentence']:\n",
    "        li.append(tokenizer.tokenize(sentence))\n",
    "    return li\n"
   ]
  },
  {
   "source": [
    "## metric"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6855113d-1ebe-4e5c-ba30-a10b5ef8b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    return acc_and_f1(preds, labels)\n",
    "\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def official_f1():\n",
    "\n",
    "    with open(os.path.join('/opt/ml/eval/result.txt'), \"r\", encoding=\"utf-8\") as f:\n",
    "        macro_result = list(f)[-1]\n",
    "        macro_result = macro_result.split(\":\")[1].replace(\">>>\", \"\").strip()\n",
    "        macro_result = macro_result.split(\"=\")[1].strip().replace(\"%\", \"\")\n",
    "        macro_result = float(macro_result) / 100\n",
    "\n",
    "    return macro_result\n",
    "\n",
    "def acc_and_f1(preds, labels, average=\"macro\"):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        #\"f1\": official_f1(),\n",
    "    }\n"
   ]
  },
  {
   "source": [
    "## add entity token"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff171a4-ad81-46e4-abe9-b019a4ef9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence_to_features(train_dataset, tokenizer, max_len):\n",
    "    \n",
    "    max_seq_len=max_len\n",
    "    cls_token=tokenizer.cls_token\n",
    "    #cls_token_segment_id=tokenizer.cls_token_id\n",
    "    cls_token_segment_id=0\n",
    "    sep_token=tokenizer.sep_token\n",
    "    pad_token=1\n",
    "    pad_token_segment_id=0\n",
    "    sequence_a_segment_id=0\n",
    "    add_sep_token=False\n",
    "    mask_padding_with_zero=True\n",
    "    \n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_token_type_ids = []\n",
    "    all_e1_mask=[]\n",
    "    all_e2_mask=[]\n",
    "    all_label=[]\n",
    "    for idx in tqdm(range(len(train_dataset))):\n",
    "        if train_dataset['e1s'][idx] > train_dataset['e2s'][idx]:\n",
    "            train_dataset['sentence'][idx] = train_dataset['sentence'][idx][:train_dataset['e2s'][idx]] + ' <e2> ' + train_dataset['sentence'][idx][train_dataset['e2s'][idx]:train_dataset['e2e'][idx]+1] + ' </e2> ' + train_dataset['sentence'][idx][train_dataset['e2e'][idx]+1:train_dataset['e1s'][idx]] + ' <e1> ' + train_dataset['sentence'][idx][train_dataset['e1s'][idx]:train_dataset['e1e'][idx]+1] + ' </e1> ' + train_dataset['sentence'][idx][train_dataset['e1e'][idx]+1:]\n",
    "        else:\n",
    "            train_dataset['sentence'][idx] = train_dataset['sentence'][idx][:train_dataset['e1s'][idx]] + ' <e1> ' + train_dataset['sentence'][idx][train_dataset['e1s'][idx]:train_dataset['e1e'][idx]+1] + ' </e1> ' + train_dataset['sentence'][idx][train_dataset['e1e'][idx]+1:train_dataset['e2s'][idx]] + ' <e2> ' + train_dataset['sentence'][idx][train_dataset['e2s'][idx]:train_dataset['e2e'][idx]+1] + ' </e2> ' + train_dataset['sentence'][idx][train_dataset['e2e'][idx]+1:]    \n",
    "\n",
    "        \n",
    "        token = tokenizer.tokenize(train_dataset['sentence'][idx])\n",
    "        \n",
    "        e11_p = token.index(\"<e1>\")  # the start position of entity1\n",
    "        e12_p = token.index(\"</e1>\")  # the end position of entity1\n",
    "        e21_p = token.index(\"<e2>\")  # the start position of entity2\n",
    "        e22_p = token.index(\"</e2>\")  # the end position of entity2\n",
    "\n",
    "        token[e11_p] = \"$\"\n",
    "        token[e12_p] = \"$\"\n",
    "        token[e21_p] = \"#\"\n",
    "        token[e22_p] = \"#\"\n",
    "\n",
    "        #print(token)\n",
    "\n",
    "        e11_p += 1\n",
    "        e12_p += 1\n",
    "        e21_p += 1\n",
    "        e22_p += 1\n",
    "\n",
    "        special_tokens_count = 1\n",
    "\n",
    "        if len(token) < max_seq_len - special_tokens_count:\n",
    "#            token = token[: (max_seq_len - special_tokens_count)]\n",
    "\n",
    "#         if add_sep_token:\n",
    "#             token += [sep_token]\n",
    "\n",
    "            token_type_ids = [sequence_a_segment_id] * len(token)\n",
    "\n",
    "            token = [cls_token] + token \n",
    "            token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "            attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "            padding_length = max_seq_len - len(input_ids)\n",
    "            input_ids = input_ids + ([pad_token] * padding_length)\n",
    "            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "            e1_mask = [0] * len(attention_mask)\n",
    "            e2_mask = [0] * len(attention_mask)\n",
    "\n",
    "            for i in range(e11_p, e12_p + 1):\n",
    "                e1_mask[i] = 1\n",
    "            for i in range(e21_p, e22_p + 1):\n",
    "                e2_mask[i] = 1\n",
    "\n",
    "            assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n",
    "            assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(\n",
    "                len(attention_mask), max_seq_len\n",
    "            )\n",
    "            assert len(token_type_ids) == max_seq_len, \"Error with token type length {} vs {}\".format(\n",
    "                len(token_type_ids), max_seq_len\n",
    "            )\n",
    "\n",
    "            all_input_ids.append(input_ids)\n",
    "            all_attention_mask.append(attention_mask)\n",
    "            all_token_type_ids.append(token_type_ids)\n",
    "            all_e1_mask.append(e1_mask)\n",
    "            all_e2_mask.append(e2_mask)\n",
    "            all_label.append(train_dataset['label'][idx])\n",
    "    \n",
    "    all_features = {\n",
    "        'input_ids' : torch.tensor(all_input_ids),\n",
    "        'attention_mask' : torch.tensor(all_attention_mask),\n",
    "        'token_type_ids' : torch.tensor(all_token_type_ids),\n",
    "        'e1_mask' : torch.tensor(all_e1_mask),\n",
    "        'e2_mask' : torch.tensor(all_e2_mask)\n",
    "    }  \n",
    "    return RE_Dataset(all_features, all_label)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6fa607d-195f-428d-a722-c6e6c1bd8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n",
    "\n",
    "# Implementation from fastai https://github.com/fastai/fastai2/blob/master/fastai2/layers.py#L338\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, e:float=0.05, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.e,self.reduction = e,reduction\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        # number of classes\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        # (1-ε)* H(q,p) + ε*H(u,p)\n",
    "        return (1-self.e)*nll + self.e*(loss/c) "
   ]
  },
  {
   "source": [
    "## R-XLM 모델 정의 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25068cb-7a20-49fc-8972-7e18830dd6b9",
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7553d1b07c1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFCLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFCLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        if self.use_activation:\n",
    "            x = self.tanh(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class RXLMRoberta(XLMRobertaModel):\n",
    "    def __init__(self,  model_name, config, dropout_rate):\n",
    "        super(RXLMRoberta, self).__init__(config)\n",
    "        self.XLMRoberta = XLMRobertaModel.from_pretrained(model_name, config=config)  # Load pretrained XLMRoberta\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, dropout_rate)\n",
    "        self.entity_fc_layer1 = FCLayer(config.hidden_size, config.hidden_size, dropout_rate)\n",
    "        self.entity_fc_layer2 = FCLayer(config.hidden_size, config.hidden_size, dropout_rate)\n",
    "\n",
    "        self.label_classifier = FCLayer(\n",
    "            config.hidden_size * 3,\n",
    "#             config.hidden_size * 4,\n",
    "            config.num_labels,\n",
    "            dropout_rate,\n",
    "            use_activation=False,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def entity_average(hidden_output, e_mask):\n",
    "        \"\"\"\n",
    "        Average the entity hidden state vectors (H_i ~ H_j)\n",
    "        :param hidden_output: [batch_size, j-i+1, dim]\n",
    "        :param e_mask: [batch_size, max_seq_len]\n",
    "                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]\n",
    "        :return: [batch_size, dim]\n",
    "        \"\"\"\n",
    "        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n",
    "        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n",
    "\n",
    "        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n",
    "        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)\n",
    "        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting\n",
    "        return avg_vector\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels, e1_mask, e2_mask):\n",
    "        outputs = self.XLMRoberta(\n",
    "            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]  # [CLS]\n",
    "    \n",
    "        e1_h = self.entity_average(sequence_output, e1_mask)\n",
    "        e2_h = self.entity_average(sequence_output, e2_mask)\n",
    "        # Dropout -> tanh -> fc_layer (Share FC layer for e1 and e2)\n",
    "        pooled_output = self.cls_fc_layer(pooled_output)\n",
    "        e1_h = self.entity_fc_layer1(e1_h)\n",
    "        e2_h = self.entity_fc_layer2(e2_h)\n",
    "        # Concat -> fc_layer\n",
    "        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-1)\n",
    "\n",
    "        logits = self.label_classifier(concat_h)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        # Softmax\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                loss_fct = nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                # loss_fct = LabelSmoothingCrossEntropy()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs, pooled_output, e1_h, e2_h  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "source": [
    "## Trainer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14e99d0c-e953-41a0-a38e-87c51f6ba795",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "class Trainer(object):\n",
    "    def __init__(self,num_labels, label_dict,logging_steps, save_steps,max_steps,\n",
    "                 num_train_epochs,warmup_steps,adam_epsilon,learning_rate,gradient_accumulation_steps,\n",
    "                 max_grad_norm, eval_batch_size, train_batch_size, model_dir, dropout_rate,\n",
    "                 weight_decay, Model_name ,train_dataset=None, dev_dataset=None, test_dataset=None):\n",
    "        #self.args = args\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.dev_dataset = dev_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.Model_name = Model_name\n",
    "        self.label_lst = label_dict\n",
    "        self.num_labels = num_labels\n",
    "        self.max_steps = max_steps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.adam_epsilon=adam_epsilon\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_train_epochs = num_train_epochs\n",
    "        self.logging_steps = logging_steps\n",
    "        self.save_steps = save_steps\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.model_dir = model_dir\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.config = XLMRobertaConfig.from_pretrained(\n",
    "            self.Model_name,\n",
    "            num_labels=self.num_labels,\n",
    "            #id2label={str(i): label for i, label in enumerate(self.label_lst)},\n",
    "            id2label=self.label_lst,\n",
    "            #label2id={label: i for key, label in self.label_lst},\n",
    "            label2id={value : key for key, value in self.label_lst.items()}\n",
    "        )\n",
    "        self.model = RXLMRoberta(\n",
    "            self.Model_name, config=self.config, dropout_rate = self.dropout_rate,\n",
    "        )\n",
    "\n",
    "        # GPU or CPU\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self):\n",
    "        train_sampler = RandomSampler(self.train_dataset)\n",
    "        train_dataloader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            batch_size=self.train_batch_size,\n",
    "        )\n",
    "\n",
    "        if self.max_steps > 0:\n",
    "            t_total = self.max_steps\n",
    "            self.num_train_epochs = (\n",
    "                self.max_steps // (len(train_dataloader) // self.gradient_accumulation_steps) + 1\n",
    "            )\n",
    "        else:\n",
    "            t_total = len(train_dataloader) // self.gradient_accumulation_steps * self.num_train_epochs\n",
    "\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=self.learning_rate,\n",
    "            eps=self.adam_epsilon,\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=t_total,\n",
    "        )\n",
    "#        scaler = torch.cuda.amp.GradScaler()\n",
    "        # Train!\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(self.train_dataset))\n",
    "        logger.info(\"  Num Epochs = %d\", self.num_train_epochs)\n",
    "        logger.info(\"  Total train batch size = %d\", self.train_batch_size)\n",
    "        logger.info(\"  Gradient Accumulation steps = %d\", self.gradient_accumulation_steps)\n",
    "        logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "        logger.info(\"  Logging steps = %d\", self.logging_steps)\n",
    "        logger.info(\"  Save steps = %d\", self.save_steps)\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss = 0.0\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        train_iterator = tqdm(range(int(self.num_train_epochs)), desc=\"Epoch\")\n",
    "\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                self.model.train()\n",
    "                batch = tuple(batch[t].to(self.device) for t in batch)  # GPU or CPU\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\" : batch[2],\n",
    "                    \"labels\": batch[5],\n",
    "                    \"e1_mask\": batch[3],\n",
    "                    \"e2_mask\": batch[4]\n",
    "                }\n",
    "#                 with torch.cuda.amp.autocast():\n",
    "                outputs, pooled_out, e1_h, e2_h = self.model(**inputs)\n",
    "                loss = outputs[0]\n",
    "\n",
    "                if self.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / self.gradient_accumulation_steps\n",
    "\n",
    "#                 scaler.scale(loss).backward()\n",
    "                loss.backward()\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % self.gradient_accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "\n",
    "                    optimizer.step()\n",
    "#                     scaler.step(optimizer)\n",
    "#                     scaler.update()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    self.model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "                    if self.logging_steps > 0 and global_step % self.logging_steps == 0:\n",
    "                        logger.info(\"  global steps = %d\", global_step)\n",
    "                        self.evaluate(\"train\")  # There is no dev set for semeval task\n",
    "\n",
    "                    if self.save_steps > 0 and global_step % self.save_steps == 0:\n",
    "                        self.save_model()\n",
    "\n",
    "                if 0 < self.max_steps < global_step:\n",
    "                    epoch_iterator.close()\n",
    "                    break\n",
    "\n",
    "            if 0 < self.max_steps < global_step:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "\n",
    "        return global_step, tr_loss / global_step\n",
    "\n",
    "    def evaluate(self, mode):\n",
    "        # We use test dataset because semeval doesn't have dev dataset\n",
    "        if mode == \"test\":\n",
    "            dataset = self.test_dataset\n",
    "        elif mode == \"dev\":\n",
    "            dataset = self.dev_dataset\n",
    "        elif mode == \"train\":\n",
    "            dataset = self.train_dataset\n",
    "        else:\n",
    "            raise Exception(\"Only dev and test dataset available\")\n",
    "\n",
    "        eval_sampler = SequentialSampler(dataset)\n",
    "        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.eval_batch_size)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation on %s dataset *****\", mode)\n",
    "        logger.info(\"  Num examples = %d\", len(dataset))\n",
    "        logger.info(\"  Batch size = %d\", self.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        preds = None\n",
    "        out_label_ids = None\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            batch = tuple(batch[t].to(self.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                    \"labels\": batch[5],\n",
    "                    \"e1_mask\": batch[3],\n",
    "                    \"e2_mask\": batch[4],\n",
    "                }\n",
    "                #with torch.cuda.amp.autocast():\n",
    "                outputs, pooled_output, e1_h, e2_h = self.model(**inputs)\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "                eval_loss += tmp_eval_loss.mean().item()\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        results = {\"loss\": eval_loss}\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "\n",
    "        result = compute_metrics(preds, out_label_ids)\n",
    "        results.update(result)\n",
    "\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(results.keys()):\n",
    "            logger.info(\"  {} = {:.4f}\".format(key, results[key]))\n",
    "        print('pooled_output :', pooled_output)\n",
    "        print('e1_h :', e1_h)\n",
    "        print('e2_h :', e2_h)\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def test_pred(self):\n",
    "        test_dataset = self.test_dataset\n",
    "        test_sampler = SequentialSampler(test_dataset)\n",
    "        test_dataloader = DataLoader(test_dataset, sampler=test_sampler,batch_size=self.eval_batch_size)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation on %s dataset *****\", \"test\")\n",
    "        #logger.info(\"  Num examples = %d\", len(dataset))\n",
    "        logger.info(\"  Batch size = %d\", self.eval_batch_size)\n",
    "\n",
    "        nb_eval_steps = 0\n",
    "        preds = None\n",
    "        out_label_ids = None\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch in tqdm(test_dataloader, desc=\"Predicting\"):\n",
    "            batch = tuple(batch[t].to(self.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                    \"labels\": None,\n",
    "                    \"e1_mask\": batch[3],\n",
    "                    \"e2_mask\": batch[4],\n",
    "                }\n",
    "                outputs, pooled_output, e1_h, e2_h = self.model(**inputs)\n",
    "                #print(outputs)\n",
    "                pred = outputs[0]\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "            if preds is None:\n",
    "                preds = pred.detach().cpu().numpy()\n",
    "                #out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, pred.detach().cpu().numpy(), axis=0)\n",
    "                #out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        df = pd.DataFrame(preds, columns=['pred'])\n",
    "        df.to_csv('RXLMRoberta_batch32_epoch6.csv', index=False)\n",
    "#         with open(\"proposed_answers.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#             for idx, pred in enumerate(preds):\n",
    "#                 f.write(\"{}\\n\".format(pred))\n",
    "        #write_prediction(self.args, os.path.join(self.args.eval_dir, \"proposed_answers.txt\"), preds)\n",
    "    \n",
    "\n",
    "    def save_model(self):\n",
    "        # Save model checkpoint (Overwrite)\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "        model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        model_to_save.save_pretrained(self.model_dir)\n",
    "\n",
    "        # Save training arguments together with the trained model\n",
    "        #torch.save(self.args, os.path.join(self.args.model_dir, \"training_args.bin\"))\n",
    "        logger.info(\"Saving model checkpoint to %s\", self.model_dir)\n",
    "\n",
    "    def load_model(self):\n",
    "        # Check whether model exists\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            raise Exception(\"Model doesn't exists! Train first!\")\n",
    "\n",
    "        #self.args = torch.load(os.path.join(self.args.model_dir, \"training_args.bin\"))\n",
    "        self.model = RXLMRoberta.from_pretrained(self.model_dir)\n",
    "        self.model.to(self.device)\n",
    "        logger.info(\"***** Model Loaded *****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdf1565f-1a5a-4def-9b07-b7a8d3611cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger():\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c16c05-f3bd-451c-9c6e-1ab30da8295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"xlm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "ADDITIONAL_SPECIAL_TOKENS = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"] \n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": ADDITIONAL_SPECIAL_TOKENS})\n",
    "\n",
    "train_dataset = load_data(\"/opt/ml/input/data/train/train.tsv\")\n",
    "train_label = train_dataset['label'].values\n",
    "\n",
    "for idx in tqdm(range(len(train_dataset))):\n",
    "    if train_dataset['e1s'][idx] > train_dataset['e2s'][idx]:\n",
    "        train_dataset['sentence'][idx] = train_dataset['sentence'][idx][:train_dataset['e2s'][idx]] + ' <e2> ' + train_dataset['sentence'][idx][train_dataset['e2s'][idx]:train_dataset['e2e'][idx]+1] + ' </e2> ' + train_dataset['sentence'][idx][train_dataset['e2e'][idx]+1:train_dataset['e1s'][idx]] + ' <e1> ' + train_dataset['sentence'][idx][train_dataset['e1s'][idx]:train_dataset['e1e'][idx]+1] + ' </e1> ' + train_dataset['sentence'][idx][train_dataset['e1e'][idx]+1:]          \n",
    "    else:\n",
    "        train_dataset['sentence'][idx] = train_dataset['sentence'][idx][:train_dataset['e1s'][idx]] + ' <e1> ' + train_dataset['sentence'][idx][train_dataset['e1s'][idx]:train_dataset['e1e'][idx]+1] + ' </e1> ' + train_dataset['sentence'][idx][train_dataset['e1e'][idx]+1:train_dataset['e2s'][idx]] + ' <e2> ' + train_dataset['sentence'][idx][train_dataset['e2s'][idx]:train_dataset['e2e'][idx]+1] + ' </e2> ' + train_dataset['sentence'][idx][train_dataset['e2e'][idx]+1:]              \n",
    "\n",
    "\n",
    "tokenized_len_dataset = tokenized_dataset_len(train_dataset, tokenizer)\n",
    "\n",
    "print('최대 길이 : ', max(len(i) for i in tokenized_len_dataset))\n",
    "print('평균 길이 : ', sum(map(len, tokenized_len_dataset))/len(tokenized_len_dataset))\n",
    "plt.hist([len(s) for s in tokenized_len_dataset], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## train and test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08cd6178-fb07-4242-a062-a0ab9eef07dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5a00582fb64d85885eb189eff1f92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29807cebb3f439a96869c78ade98724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 02:45:33 - INFO - __main__ -   ***** Running training *****\n",
      "04/22/2021 02:45:33 - INFO - __main__ -     Num examples = 9000\n",
      "04/22/2021 02:45:33 - INFO - __main__ -     Num Epochs = 7\n",
      "04/22/2021 02:45:33 - INFO - __main__ -     Total train batch size = 16\n",
      "04/22/2021 02:45:33 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "04/22/2021 02:45:33 - INFO - __main__ -     Total optimization steps = 3941\n",
      "04/22/2021 02:45:33 - INFO - __main__ -     Logging steps = 400\n",
      "04/22/2021 02:45:33 - INFO - __main__ -     Save steps = 400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbe3abdca4742a49bd409a7632c1d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=7.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f55d4731c67435393e36c0c5eceae59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=563.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "04/22/2021 02:53:07 - INFO - __main__ -     global steps = 400\n",
      "04/22/2021 02:53:07 - INFO - __main__ -   ***** Running evaluation on train dataset *****\n",
      "04/22/2021 02:53:07 - INFO - __main__ -     Num examples = 9000\n",
      "04/22/2021 02:53:07 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e54cfd6c00494fb102fcd972c82f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=563.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 02:56:31 - INFO - __main__ -   ***** Eval results *****\n",
      "04/22/2021 02:56:31 - INFO - __main__ -     acc = 0.7033\n",
      "04/22/2021 02:56:31 - INFO - __main__ -     loss = 1.0169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pooled_output : tensor([[ 0.5968, -0.5054,  0.0080,  ..., -0.5293,  0.4743, -0.1478],\n",
      "        [ 0.4530, -0.3496,  0.2945,  ..., -0.2286,  0.4718, -0.4291],\n",
      "        [ 0.4187, -0.3692,  0.2007,  ..., -0.4516,  0.3265,  0.0399],\n",
      "        ...,\n",
      "        [ 0.5758, -0.3512, -0.0151,  ..., -0.5082,  0.4539, -0.1820],\n",
      "        [ 0.4301, -0.3186,  0.2749,  ..., -0.3603,  0.2721, -0.0439],\n",
      "        [ 0.4029, -0.3384,  0.0713,  ..., -0.4913,  0.4758, -0.0841]],\n",
      "       device='cuda:0')\n",
      "e1_h : tensor([[ 0.5505, -0.5411,  0.1534,  ..., -0.3062,  0.3908, -0.4984],\n",
      "        [ 0.3242, -1.2876, -0.4255,  ...,  0.9669,  0.5433,  0.5812],\n",
      "        [ 0.3715, -1.0845, -0.2710,  ...,  0.8746,  0.2560,  0.6337],\n",
      "        ...,\n",
      "        [ 0.2977,  0.7564,  0.5662,  ..., -0.7824,  0.0853, -0.0808],\n",
      "        [ 0.1620, -1.2865, -0.6557,  ...,  0.6354,  0.1534,  0.6001],\n",
      "        [ 0.0680,  0.5687,  0.2027,  ..., -0.5986, -0.2477, -0.9112]],\n",
      "       device='cuda:0')\n",
      "e2_h : tensor([[-0.5341, -0.8993, -0.2741,  ...,  0.1610, -0.9896, -0.1486],\n",
      "        [ 0.1927,  0.5435, -0.2077,  ..., -0.0940, -0.5953, -0.2776],\n",
      "        [-0.3590, -1.3290, -0.2475,  ...,  0.2232, -0.7642,  0.1915],\n",
      "        ...,\n",
      "        [-0.6017, -0.7884, -0.2022,  ...,  0.3966, -0.2959,  0.1200],\n",
      "        [ 0.3228,  0.6312,  0.4146,  ...,  0.2540, -0.0929, -0.7332],\n",
      "        [-0.4484, -1.3097, -0.4430,  ...,  0.1361, -0.6962,  0.1027]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 02:57:11 - INFO - __main__ -   Saving model checkpoint to ./model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2295c53a013c459badcbed95f187297d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=563.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:04:43 - INFO - __main__ -     global steps = 800\n",
      "04/22/2021 03:04:43 - INFO - __main__ -   ***** Running evaluation on train dataset *****\n",
      "04/22/2021 03:04:43 - INFO - __main__ -     Num examples = 9000\n",
      "04/22/2021 03:04:43 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5345b8e8788c490391a1abee1dbd62c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=563.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:08:07 - INFO - __main__ -   ***** Eval results *****\n",
      "04/22/2021 03:08:07 - INFO - __main__ -     acc = 0.7693\n",
      "04/22/2021 03:08:07 - INFO - __main__ -     loss = 0.7430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pooled_output : tensor([[ 0.6581, -0.2344,  0.0840,  ..., -0.6065,  0.4897, -0.0220],\n",
      "        [ 0.4281, -0.2740,  0.0598,  ..., -0.0704,  0.2414, -0.3398],\n",
      "        [ 0.2673, -0.1793,  0.2215,  ..., -0.3442,  0.1365,  0.3185],\n",
      "        ...,\n",
      "        [ 0.5919, -0.1365, -0.1544,  ..., -0.6103,  0.3068, -0.1805],\n",
      "        [ 0.5343, -0.1035,  0.4428,  ..., -0.5900,  0.4626, -0.1049],\n",
      "        [ 0.3976, -0.0629,  0.0064,  ..., -0.3281,  0.3537,  0.0595]],\n",
      "       device='cuda:0')\n",
      "e1_h : tensor([[ 0.2353, -0.6094,  0.3052,  ..., -0.5162,  0.4120, -0.5848],\n",
      "        [ 0.1684, -1.3722, -0.3361,  ...,  1.1745,  0.2914,  0.7853],\n",
      "        [ 0.1688, -1.0758, -0.2662,  ...,  1.2422, -0.0977,  0.5906],\n",
      "        ...,\n",
      "        [-0.0810,  1.4604,  0.7029,  ..., -0.9437,  0.1642, -0.3405],\n",
      "        [-0.0507, -0.9674, -0.2984,  ...,  0.4828,  0.0368,  0.6702],\n",
      "        [-0.3293,  0.6465,  0.4315,  ..., -1.0386, -0.1625, -0.8286]],\n",
      "       device='cuda:0')\n",
      "e2_h : tensor([[-0.3549, -0.7447, -0.4044,  ...,  0.1042, -0.9165, -0.2343],\n",
      "        [-0.0292,  0.9925, -0.4891,  ..., -0.0465, -0.5532, -0.6460],\n",
      "        [-0.0280, -1.3306, -0.2329,  ..., -0.3998, -0.4278,  0.1234],\n",
      "        ...,\n",
      "        [-0.7628, -0.2891, -0.2773,  ...,  0.3354, -0.5624, -0.0656],\n",
      "        [ 0.0514,  0.8215,  0.2727,  ..., -0.0197, -0.0672, -1.0023],\n",
      "        [-0.4734, -1.0500, -0.1914,  ...,  0.2394, -0.8665, -0.0795]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:08:47 - INFO - __main__ -   Saving model checkpoint to ./model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8735f0a3fa5f4895b293ca11101c3bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=563.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:16:19 - INFO - __main__ -     global steps = 1200\n",
      "04/22/2021 03:16:19 - INFO - __main__ -   ***** Running evaluation on train dataset *****\n",
      "04/22/2021 03:16:19 - INFO - __main__ -     Num examples = 9000\n",
      "04/22/2021 03:16:19 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80cbfc657fa441a85a5cb0120afdb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=563.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:19:43 - INFO - __main__ -   ***** Eval results *****\n",
      "04/22/2021 03:19:43 - INFO - __main__ -     acc = 0.8190\n",
      "04/22/2021 03:19:43 - INFO - __main__ -     loss = 0.5991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pooled_output : tensor([[ 0.7535, -0.3215,  0.0445,  ..., -0.6062,  0.4159, -0.1645],\n",
      "        [ 0.5677, -0.3433,  0.2354,  ...,  0.0214,  0.0991, -0.3114],\n",
      "        [ 0.4281, -0.0065,  0.0855,  ..., -0.3890,  0.0505,  0.0607],\n",
      "        ...,\n",
      "        [ 0.6271,  0.1255, -0.3487,  ..., -0.3638,  0.2292, -0.3673],\n",
      "        [ 0.4887, -0.0963,  0.5278,  ..., -0.6200,  0.4500,  0.0451],\n",
      "        [ 0.4306, -0.0264, -0.1056,  ..., -0.3961,  0.1768, -0.0402]],\n",
      "       device='cuda:0')\n",
      "e1_h : tensor([[-0.1084, -0.5924,  0.4787,  ..., -0.3546,  0.2150, -0.2721],\n",
      "        [ 0.0126, -1.0812, -0.1373,  ...,  1.1129,  0.2195,  0.6437],\n",
      "        [-0.0100, -0.7909, -0.0222,  ...,  0.7561, -0.3759,  0.3617],\n",
      "        ...,\n",
      "        [ 0.2276,  1.0066,  0.7921,  ..., -0.8069, -0.3589,  0.3643],\n",
      "        [-0.1453, -1.2724, -0.3624,  ...,  0.5785,  0.2252,  0.4122],\n",
      "        [-0.5824,  1.1988,  0.3402,  ..., -1.0080, -0.6178, -1.2387]],\n",
      "       device='cuda:0')\n",
      "e2_h : tensor([[-0.3610, -1.2730, -0.1089,  ...,  0.0132, -1.0213,  0.1045],\n",
      "        [-0.1927,  1.1609, -0.5707,  ..., -0.1146, -0.5373, -0.4696],\n",
      "        [-0.0660, -1.6381, -0.1494,  ..., -0.6728, -0.5091,  0.4552],\n",
      "        ...,\n",
      "        [-0.9475, -0.3138, -0.4329,  ...,  0.4903, -0.6764,  0.0660],\n",
      "        [ 0.0662,  0.6628,  0.4405,  ...,  0.2354, -0.1302, -0.8882],\n",
      "        [-0.5181, -1.3954, -0.0759,  ...,  0.1800, -0.7124, -0.0923]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:20:22 - INFO - __main__ -   Saving model checkpoint to ./model\n",
      "04/22/2021 03:27:55 - INFO - __main__ -     global steps = 1600\n",
      "04/22/2021 03:27:55 - INFO - __main__ -   ***** Running evaluation on train dataset *****\n",
      "04/22/2021 03:27:55 - INFO - __main__ -     Num examples = 9000\n",
      "04/22/2021 03:27:55 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1232eb76cce44b4a9e6f736000362f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=563.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:31:19 - INFO - __main__ -   ***** Eval results *****\n",
      "04/22/2021 03:31:19 - INFO - __main__ -     acc = 0.8682\n",
      "04/22/2021 03:31:19 - INFO - __main__ -     loss = 0.4188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pooled_output : tensor([[ 0.8186, -0.2808,  0.0989,  ..., -0.6552,  0.5334,  0.0288],\n",
      "        [ 0.6166, -0.4265,  0.2989,  ...,  0.1135,  0.0884, -0.1612],\n",
      "        [ 0.6947, -0.0023,  0.0635,  ..., -0.7729,  0.3722,  0.2724],\n",
      "        ...,\n",
      "        [ 0.8228, -0.1142, -0.2400,  ..., -0.5852,  0.3535, -0.2090],\n",
      "        [ 0.5594, -0.1175,  0.4448,  ..., -0.6512,  0.5627,  0.0359],\n",
      "        [ 0.5707, -0.0398, -0.0609,  ..., -0.6049,  0.4290,  0.1648]],\n",
      "       device='cuda:0')\n",
      "e1_h : tensor([[ 0.0019, -0.3107,  0.5463,  ..., -0.8047,  0.4300, -0.6708],\n",
      "        [-0.1843, -1.1899, -0.2040,  ...,  1.3292, -0.0085,  0.5716],\n",
      "        [ 0.1798, -1.1125, -0.3261,  ...,  0.9001, -0.3968,  0.1711],\n",
      "        ...,\n",
      "        [ 0.1720,  1.5470,  1.0137,  ..., -1.1367,  0.1014, -0.2584],\n",
      "        [-0.2860, -1.1639, -0.5402,  ...,  0.5088,  0.3157,  0.2363],\n",
      "        [-0.4022,  0.9371,  0.4668,  ..., -0.9256, -0.5298, -1.2304]],\n",
      "       device='cuda:0')\n",
      "e2_h : tensor([[-0.4566, -1.0263, -0.1987,  ...,  0.2596, -1.3083, -0.1367],\n",
      "        [-0.2534,  1.0966, -0.9042,  ..., -0.1960, -0.4812, -0.4220],\n",
      "        [-0.3037, -1.5909, -0.2182,  ..., -0.2051, -0.8972,  0.2125],\n",
      "        ...,\n",
      "        [-0.7909, -0.5743, -0.4640,  ...,  0.6368, -0.9321, -0.0610],\n",
      "        [ 0.1544,  0.4747,  0.4972,  ...,  0.1723, -0.1930, -0.7173],\n",
      "        [-0.7275, -1.1387, -0.0524,  ...,  0.2960, -0.8921, -0.1667]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:31:59 - INFO - __main__ -   Saving model checkpoint to ./model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9382806de111445c80a793c8b1fa222e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=563.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:39:31 - INFO - __main__ -     global steps = 2000\n",
      "04/22/2021 03:39:31 - INFO - __main__ -   ***** Running evaluation on train dataset *****\n",
      "04/22/2021 03:39:31 - INFO - __main__ -     Num examples = 9000\n",
      "04/22/2021 03:39:31 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87434bc836c434b9d0212d2587c0dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=563.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:42:55 - INFO - __main__ -   ***** Eval results *****\n",
      "04/22/2021 03:42:55 - INFO - __main__ -     acc = 0.9056\n",
      "04/22/2021 03:42:55 - INFO - __main__ -     loss = 0.2985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pooled_output : tensor([[ 0.8176, -0.3920,  0.2217,  ..., -0.6024,  0.5165,  0.0835],\n",
      "        [ 0.5334, -0.4229,  0.3576,  ...,  0.1207,  0.0639, -0.2713],\n",
      "        [ 0.6848, -0.0250,  0.2294,  ..., -0.7456,  0.3270,  0.2589],\n",
      "        ...,\n",
      "        [ 0.7523, -0.1879, -0.0567,  ..., -0.6522,  0.4959, -0.1301],\n",
      "        [ 0.4913, -0.1849,  0.3749,  ..., -0.6649,  0.5900,  0.1919],\n",
      "        [ 0.6099, -0.1199,  0.0798,  ..., -0.7757,  0.4415,  0.1271]],\n",
      "       device='cuda:0')\n",
      "e1_h : tensor([[ 0.1444, -0.6281,  0.5188,  ..., -0.5596,  0.5045, -0.8811],\n",
      "        [-0.2323, -1.2641, -0.2067,  ...,  1.0791,  0.2157,  0.6140],\n",
      "        [-0.0731, -0.7546, -0.4102,  ...,  0.5702, -0.5256,  0.2819],\n",
      "        ...,\n",
      "        [-0.0663,  1.8056,  0.7765,  ..., -1.0596, -0.2203, -0.5470],\n",
      "        [ 0.0543, -1.5702, -0.6345,  ...,  0.3415,  0.6090,  0.4580],\n",
      "        [-0.4625,  1.1551,  0.5151,  ..., -1.0770, -0.7554, -1.5600]],\n",
      "       device='cuda:0')\n",
      "e2_h : tensor([[-0.4123, -0.7966, -0.3642,  ...,  0.1862, -1.1042, -0.0129],\n",
      "        [-0.1064,  1.1636, -0.5540,  ..., -0.2323, -0.6763, -0.4451],\n",
      "        [ 0.0365, -1.5177, -0.3058,  ..., -0.6529, -0.7835,  0.2420],\n",
      "        ...,\n",
      "        [-0.8469, -0.3866, -0.3304,  ...,  0.5919, -0.8821, -0.0352],\n",
      "        [ 0.0393,  0.8684,  0.4633,  ...,  0.1266,  0.0171, -0.7977],\n",
      "        [-0.5969, -1.0203, -0.1859,  ...,  0.3380, -0.8755, -0.3087]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:43:36 - INFO - __main__ -   Saving model checkpoint to ./model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9a763ab7b446b6a86f93eb4671710f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=563.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:51:08 - INFO - __main__ -     global steps = 2400\n",
      "04/22/2021 03:51:08 - INFO - __main__ -   ***** Running evaluation on train dataset *****\n",
      "04/22/2021 03:51:08 - INFO - __main__ -     Num examples = 9000\n",
      "04/22/2021 03:51:08 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ebb796074f430db6a1e8cfc67129b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=563.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:54:32 - INFO - __main__ -   ***** Eval results *****\n",
      "04/22/2021 03:54:32 - INFO - __main__ -     acc = 0.9312\n",
      "04/22/2021 03:54:32 - INFO - __main__ -     loss = 0.2240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pooled_output : tensor([[ 0.8404, -0.4578,  0.2255,  ..., -0.6571,  0.4811,  0.0487],\n",
      "        [ 0.4038, -0.4052,  0.3010,  ...,  0.2636, -0.1798, -0.3236],\n",
      "        [ 0.5864, -0.0917,  0.1579,  ..., -0.7443,  0.2466,  0.2724],\n",
      "        ...,\n",
      "        [ 0.6857, -0.1172, -0.1259,  ..., -0.6888,  0.4404, -0.1799],\n",
      "        [ 0.4070, -0.0824,  0.2799,  ..., -0.5680,  0.5298,  0.0068],\n",
      "        [ 0.4907, -0.0868,  0.0391,  ..., -0.6671,  0.3015,  0.1493]],\n",
      "       device='cuda:0')\n",
      "e1_h : tensor([[-0.0593, -0.6347,  0.4069,  ..., -0.6576,  0.5627, -0.8804],\n",
      "        [-0.3956, -0.9606, -0.1419,  ...,  1.2423,  0.0748,  0.5300],\n",
      "        [-0.0702, -0.8515, -0.5740,  ...,  0.8102, -0.3849,  0.3562],\n",
      "        ...,\n",
      "        [-0.2803,  1.7075,  0.6657,  ..., -1.1887, -0.0314, -0.6035],\n",
      "        [-0.1222, -1.6182, -0.5178,  ...,  0.4757,  0.3657,  0.3763],\n",
      "        [-0.5585,  1.0434,  0.5321,  ..., -0.9820, -0.6864, -1.3030]],\n",
      "       device='cuda:0')\n",
      "e2_h : tensor([[-0.4577, -0.9023, -0.3529,  ...,  0.2427, -1.1243, -0.1254],\n",
      "        [-0.1933,  1.0179, -0.5109,  ..., -0.3752, -0.5214, -0.4280],\n",
      "        [-0.1517, -1.6004, -0.2709,  ..., -0.0623, -0.9679,  0.1776],\n",
      "        ...,\n",
      "        [-0.9790, -0.4638, -0.3692,  ...,  0.7178, -0.9356,  0.0663],\n",
      "        [-0.0627,  0.6200,  0.4272,  ...,  0.0858, -0.1528, -0.7466],\n",
      "        [-0.6809, -0.9403, -0.1389,  ...,  0.5677, -0.9217, -0.3185]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 03:55:13 - INFO - __main__ -   Saving model checkpoint to ./model\n",
      "04/22/2021 04:02:46 - INFO - __main__ -     global steps = 2800\n",
      "04/22/2021 04:02:46 - INFO - __main__ -   ***** Running evaluation on train dataset *****\n",
      "04/22/2021 04:02:46 - INFO - __main__ -     Num examples = 9000\n",
      "04/22/2021 04:02:46 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a75663df18b455db26d6f193d791fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=563.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 04:06:10 - INFO - __main__ -   ***** Eval results *****\n",
      "04/22/2021 04:06:10 - INFO - __main__ -     acc = 0.9509\n",
      "04/22/2021 04:06:10 - INFO - __main__ -     loss = 0.1596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pooled_output : tensor([[ 0.7936, -0.3344,  0.1721,  ..., -0.6009,  0.4740, -0.0844],\n",
      "        [ 0.4370, -0.4697,  0.3252,  ...,  0.2112, -0.2153, -0.2338],\n",
      "        [ 0.6847, -0.0164,  0.1972,  ..., -0.8752,  0.2856,  0.2529],\n",
      "        ...,\n",
      "        [ 0.8427, -0.2845, -0.1704,  ..., -0.5606,  0.3923, -0.1844],\n",
      "        [ 0.4575, -0.0668,  0.3932,  ..., -0.7471,  0.5394,  0.2785],\n",
      "        [ 0.5913, -0.1253,  0.0394,  ..., -0.7463,  0.3426,  0.1734]],\n",
      "       device='cuda:0')\n",
      "e1_h : tensor([[ 0.1448, -0.8580,  0.5297,  ..., -0.5481,  0.7991, -0.6960],\n",
      "        [-0.3800, -1.2266, -0.1960,  ...,  1.3542, -0.0660,  0.4130],\n",
      "        [-0.1704, -0.9703, -0.3703,  ...,  0.6817, -0.7358,  0.2599],\n",
      "        ...,\n",
      "        [ 0.4184,  1.2766,  0.3069,  ..., -0.9861, -0.0937,  0.1626],\n",
      "        [-0.1509, -1.7451, -0.5666,  ...,  0.3891,  0.4462,  0.5085],\n",
      "        [-0.5024,  0.9477,  0.3202,  ..., -1.1230, -0.6363, -1.3302]],\n",
      "       device='cuda:0')\n",
      "e2_h : tensor([[-0.3891, -0.8564, -0.0783,  ...,  0.0666, -1.0446, -0.0315],\n",
      "        [-0.5465,  1.1146, -0.7520,  ..., -0.3216, -0.3563, -0.2857],\n",
      "        [-0.0717, -1.7591, -0.3714,  ..., -0.7089, -0.6436,  0.3425],\n",
      "        ...,\n",
      "        [-0.9785, -0.5577, -0.4079,  ...,  0.5269, -0.9630,  0.1694],\n",
      "        [-0.0260,  0.6225,  0.5199,  ...,  0.0547, -0.0442, -0.7336],\n",
      "        [-0.5828, -1.3151, -0.0674,  ...,  0.1312, -0.9562, -0.2271]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 04:06:50 - INFO - __main__ -   Saving model checkpoint to ./model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7b297a9740485183e5e935de4cc5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=563.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 04:14:21 - INFO - __main__ -     global steps = 3200\n",
      "04/22/2021 04:14:21 - INFO - __main__ -   ***** Running evaluation on train dataset *****\n",
      "04/22/2021 04:14:21 - INFO - __main__ -     Num examples = 9000\n",
      "04/22/2021 04:14:21 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3316c75170242e49dd628c5cb5da8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=563.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 04:17:45 - INFO - __main__ -   ***** Eval results *****\n",
      "04/22/2021 04:17:45 - INFO - __main__ -     acc = 0.9627\n",
      "04/22/2021 04:17:45 - INFO - __main__ -     loss = 0.1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pooled_output : tensor([[ 0.8096, -0.4181,  0.3862,  ..., -0.7160,  0.5060, -0.0835],\n",
      "        [ 0.4113, -0.4847,  0.3711,  ...,  0.2803, -0.1870, -0.4018],\n",
      "        [ 0.5878, -0.0195,  0.3517,  ..., -0.9885,  0.2133,  0.3750],\n",
      "        ...,\n",
      "        [ 0.7636, -0.2945, -0.1067,  ..., -0.5956,  0.4393, -0.2504],\n",
      "        [ 0.4968, -0.1110,  0.3976,  ..., -0.7423,  0.4673,  0.2778],\n",
      "        [ 0.5755, -0.1698,  0.0300,  ..., -0.8564,  0.4580,  0.0946]],\n",
      "       device='cuda:0')\n",
      "e1_h : tensor([[ 0.1996, -0.8950,  0.5629,  ..., -0.4823,  0.8702, -0.4783],\n",
      "        [-0.4376, -1.2436, -0.3120,  ...,  1.4136,  0.1521,  0.4850],\n",
      "        [-0.0582, -0.8727, -0.5422,  ...,  0.7598, -0.5221,  0.5216],\n",
      "        ...,\n",
      "        [ 0.5166,  0.9645,  0.5569,  ..., -0.9223, -0.0121,  0.5389],\n",
      "        [ 0.0734, -1.9136, -0.3754,  ...,  0.2714,  0.6256,  0.4943],\n",
      "        [-0.5747,  0.9187,  0.2857,  ..., -0.9010, -0.4814, -1.4227]],\n",
      "       device='cuda:0')\n",
      "e2_h : tensor([[-0.4089, -0.2667,  0.0472,  ...,  0.1467, -0.7755, -0.3872],\n",
      "        [-0.3934,  1.2469, -0.7548,  ..., -0.4260, -0.4048, -0.3868],\n",
      "        [-0.1077, -1.8658, -0.3988,  ..., -0.7501, -0.7566,  0.4202],\n",
      "        ...,\n",
      "        [-0.8434, -0.6606, -0.3266,  ...,  0.5974, -0.9052,  0.0542],\n",
      "        [-0.1050,  0.7691,  0.5666,  ..., -0.0057, -0.0115, -0.8153],\n",
      "        [-0.6746, -1.0282, -0.1032,  ...,  0.4235, -0.8828, -0.3662]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 04:18:25 - INFO - __main__ -   Saving model checkpoint to ./model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe567aa69fc479385f0527af59fd1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=563.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 04:25:56 - INFO - __main__ -     global steps = 3600\n",
      "04/22/2021 04:25:56 - INFO - __main__ -   ***** Running evaluation on train dataset *****\n",
      "04/22/2021 04:25:56 - INFO - __main__ -     Num examples = 9000\n",
      "04/22/2021 04:25:56 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a7342c77a74290b8494f9a518c2436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=563.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 04:29:20 - INFO - __main__ -   ***** Eval results *****\n",
      "04/22/2021 04:29:20 - INFO - __main__ -     acc = 0.9740\n",
      "04/22/2021 04:29:20 - INFO - __main__ -     loss = 0.0868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pooled_output : tensor([[ 0.8549, -0.5302,  0.4138,  ..., -0.6453,  0.5808, -0.1457],\n",
      "        [ 0.3566, -0.4800,  0.3962,  ...,  0.2642, -0.2407, -0.3287],\n",
      "        [ 0.7002, -0.1426,  0.3379,  ..., -0.9770,  0.3218,  0.2485],\n",
      "        ...,\n",
      "        [ 0.7104, -0.2818, -0.0948,  ..., -0.6822,  0.4789, -0.2504],\n",
      "        [ 0.5667, -0.1954,  0.3829,  ..., -0.5048,  0.4701,  0.0155],\n",
      "        [ 0.5825, -0.1652, -0.0272,  ..., -0.7675,  0.4000,  0.0550]],\n",
      "       device='cuda:0')\n",
      "e1_h : tensor([[ 0.2464, -1.1756,  0.4737,  ..., -0.2276,  0.7984, -0.0793],\n",
      "        [-0.3920, -1.0273, -0.2345,  ...,  1.4161,  0.1150,  0.3971],\n",
      "        [-0.2375, -0.9537, -0.4399,  ...,  0.7392, -0.9767,  0.3790],\n",
      "        ...,\n",
      "        [ 0.3824,  1.0792,  0.4222,  ..., -0.9337, -0.0916,  0.3177],\n",
      "        [ 0.3011, -1.9317, -0.2623,  ...,  0.1824,  0.6252,  0.3822],\n",
      "        [-0.4378,  1.1192,  0.3370,  ..., -0.9300, -0.7362, -1.2873]],\n",
      "       device='cuda:0')\n",
      "e2_h : tensor([[-0.3790, -0.3196,  0.1227,  ...,  0.1118, -0.7269, -0.3694],\n",
      "        [-0.5115,  1.1348, -0.7017,  ..., -0.3952, -0.3907, -0.4078],\n",
      "        [-0.1500, -1.7571, -0.3380,  ..., -0.6345, -0.8001,  0.3815],\n",
      "        ...,\n",
      "        [-0.9154, -0.5181, -0.3350,  ...,  0.6211, -0.9360,  0.0329],\n",
      "        [-0.1236,  0.7946,  0.6355,  ...,  0.0284,  0.0260, -0.8248],\n",
      "        [-0.7175, -0.9148, -0.1448,  ...,  0.5595, -0.8748, -0.4088]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2021 04:30:01 - INFO - __main__ -   Saving model checkpoint to ./model\n",
      "04/22/2021 04:36:27 - INFO - __main__ -   ***** Running evaluation on test dataset *****\n",
      "04/22/2021 04:36:27 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf2c4c7b9834be49435d3b725d03b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Predicting', max=63.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    init_logger()\n",
    "    train_dataset = load_data(\"/opt/ml/input/data/train/train_original.tsv\")\n",
    "    test_dataset = load_data(\"/opt/ml/input/data/test/test.tsv\")\n",
    "\n",
    "    ADDITIONAL_SPECIAL_TOKENS = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\n",
    "    MODEL_NAME = \"xlm-roberta-large\"\n",
    "\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": ADDITIONAL_SPECIAL_TOKENS})\n",
    "      \n",
    "    \n",
    "    train_Dataset = convert_sentence_to_features(train_dataset, tokenizer, max_len = 345+2)\n",
    "    test_Dataset = convert_sentence_to_features(test_dataset, tokenizer, max_len=345+2)\n",
    "    with open('/opt/ml/input/data/label_type.pkl', 'rb') as f:\n",
    "        label_type = pickle.load(f)\n",
    "    \n",
    "    trainer = Trainer(eval_batch_size=16,train_batch_size=16, num_labels = 42,\n",
    "                      max_steps=-1, weight_decay=0.0, learning_rate= 2e-5, \n",
    "                      adam_epsilon=1e-8, warmup_steps=0, num_train_epochs=7,\n",
    "                      logging_steps=400, save_steps=400, max_grad_norm=1.0, \n",
    "                      model_dir='./model', gradient_accumulation_steps=1,dropout_rate = 0.1,\n",
    "                      label_dict=label_type,Model_name=MODEL_NAME,train_dataset=train_Dataset,\n",
    "                      test_dataset=test_Dataset)\n",
    "    \n",
    "    do_train = True\n",
    "    do_test = True\n",
    "    if do_train:\n",
    "        trainer.train()\n",
    "\n",
    "    if do_test:\n",
    "        trainer.test_pred()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}